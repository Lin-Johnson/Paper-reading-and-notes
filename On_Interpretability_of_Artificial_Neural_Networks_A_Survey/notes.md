## Ⅰ INTRODUCTION

### A. What Does Interpretability Mean?

Generally speaking, interpretability refers to the extent of human‘s ability to understand and reason a model.

一般来说，可解释性是指人类理解和推理模型的能力的程度。（可解释性就是用合理的有依据的方法让人类相信你的模型是正确的）

- **Simulatability is considered as the understanding over the entire model.**

  用简单的模型去模拟复杂的神经网络（模型越简单，模型越容易解释，模型的可解释性越高）

- **Decomposability is to understand a model in terms of its components, such as neurons, layers, blocks, and so on.**

  将复杂的神经网络分解为多个模块，分析各个模块的功能和可解释性。

- **Algorithmic transparency is to understand the training process and dynamics of a model.** 

  神经网络缺乏透明度体现为：

  - 算法不可解释
  - 训练数据集缺乏可视性（数据来源、数据处理、训练模型的特征）
  - 数据选择方式不透明
  - 对训练数据集中偏见的理解有限
  - 模型版本化的透明度有限






### B. Why Is Interpretability Difficult?

**1. Human Limitation能力限制**

Expertise is often insufficient in many applications. Nowadays, deep learning has been extensively used in tackling intricate problems, which even professionals are unable to comprehend adequately.

简单来说就是有些深度学习的结果没有人能证明是怎么得来的

**2. Commercial Barrier商业壁垒**

First and foremost, companies profit from black-box models. It is not a common practice that a company makes capital out of totally transparent models. Second, model opacity helps protect hard work from being reverse engineered. An effective black box is ideal in the sense that customers being served can obtain satisfactory results while competitors are not able to steal their intellectual properties easily. Third, prototyping an interpretable model may cost too much in terms of financial, computational, and other resources. 

我不太清楚为什么要扯上商业去，他给出的解释为：

- 公司从不可解释的模型中获利，但是不会用完全透明的模式来生产资本
- 模型的不透明度有利于保护工作使得竞争对手不容易窃取到知识产权
- 构建一个可解释模型在财务、计算和其他资源方面花费太高，不值得

**3. Data Wildness数据紊乱**

- On the one hand, although it is a big data era, high-quality data are often not accessible in many domains. 

  **高度异质性**和**不一致**的数据阻碍了深度学习模型的准确性的同时而也阻碍了可解释性的构建。

- On the other hand, real-world data have the character of high dimensionality, which suppresses reasoning.

  神经网络训练模型过程中存在维度映射，深度学习模型输入变量的数量过大。

**4. Algorithmic Complexity算法复杂**

Deep learning is a kind of large scale and highly nonlinear algorithms. Convolution, pooling, nonlinear activation, shortcut, and so on contribute to the variability of neural networks. 

深度学习是一种大尺度的高度非线性算法。卷积、池化、非线性激活、快捷方式等都有助于促进神经网络的可变性。**深度学习的算法和参数的调节都会引起神经网络可解释性的改变。**

Recursiveness is another source of difficulty.

递归性是困难的另一个来源，**初始输入的微小变化可能会导致巨大的结果差异**，从而增加了解释方法的复杂性。





### C. How to Build Good Interpretation Method?

第三个主要问题是评估所提出的解释方法的质量的标准。由于现有的评价方法还不成熟，提出了五种一般的、定义良好的经验法则：

**1. Exactness精确性（准确性）**

一般来说，定量解释方法比定性解释方法更可取

|      | **定量研究**  |   **定性研究**   |
| :--: | :-----------: | :--------------: |
|  1   |   检验假设    |     提出假设     |
|  2   |  用数字表示   |    用文字表达    |
|  3   | 较大的样本量  |   较小的样本量   |
|  4   | 数学/统计分析 | 总结、分类、解释 |

如果目标是证实或检验理论或假设，很可能会选择定量方法；如果想理解或探索一个想法，应该采用定性的方法。

**2. Consistency一致性**

一致性表明，在一个解释中不存在矛盾。对于多个相似的样本，一个公平的解释应该能产生一致的答案。此外，一种解释方法应与真实模型的预测相一致。

**3. Completeness完整性**

从数学上讲，神经网络是为了学习最适合数据的映射。一个好的解释方法应该显示支持最大数据实例和数据类型的有效性。即一个解释方法应该尽可能完整地解释整个神经网络。

**4. Universality普遍性**

随着深度学习快速发展，大大丰富了深度学习的学习方法，一个好的解释方法应该尽可能多的去解释更多模型，节省劳动力和时间。

**5. Reward奖励性**

对深度学习的深入理解将有助于神经网络的研究和应用。







## Ⅱ  SURVEY ON INTERPRETATION METHODS

**A. Taxonomy Definition分类定义**

**B. Post-Hoc Interpretability Analysis事后可解释性分析**

![Interpretation Methods](images/Interpretation Methods.png)

**备注：以下部分为机翻内容，未经修改，仅供参考**

### Post-hoc interpretability

Post-hoc interpretability is conducted after a model is well learned. A main advantage of *post-hoc* methods is that one does not need to compromise interpretability with the predictive performance since prediction and interpretation are two separate processes with out mutual interference. However, a *post-hoc* interpretation is usually not completely faithful to the original model. If an interpretation is 100% accurate compared to the original model, it becomes the original model.

事后的可解释性是在对一个模型进行了充分的学习之后进行的。事后方法的一个主要优点是，不需要降低可解释性，因为预测和解释是两个独立的过程，没有相互干扰。然而，一个事后的解释通常并不完全忠实于原始的模型。如果一个解释与原始模型相比是100%准确的，那么它就成为了原始模型。

Therefore, any interpretation method in this category is more or less inaccurate. What is worse is that we often do not know the nuance. Such a nuance makes it hard for practitioners to have a full trust to an interpretation method, because the correctness of the interpretation method is not guaranteed.

因此，这类人的解释方法或多或少是不准确的。更糟糕的是，我们经常不知道其细微差别。这种细微的差别使从业者很难完全信任一种解释方法，因为解释方法的正确性不能得到保证。



#### 1. Feature analysis特征分析

特征分析技术主要集中于比较、分析和可视化神经元和神经元层的特征。通过特征分析，识别出了敏感的特征和处理它们的方法，从而在一定程度上解释了模型的基本原理。

特征分析技术可以应用于任何神经网络，并提供关于网络学习到哪些特征的定性见解。然而，这些技术缺乏深入、严格和统一的理解，因此不能用于修改模型，以获得更高的可解释性。



#### 2. Model inspection模型检验

模型检验方法利用外部算法，通过系统地提取神经网络内部工作机制的重要结构和参数信息，利用外部算法深入研究神经网络。

这类方法比特性分析中的方法在技术上更负责任，因为像统计数据等分析工具直接涉及到性能分析过程中。因此，通过模型检验方法获得的信息更可信和有益。在一个示例性的研究中，找到重要的数据路由路径被用作理解模型的一种方法。有了这样的数据路由路径，模型可以忠实地压缩成一个紧凑的模型。换句话说，可解释性提高了模型压缩的可信度。



#### 3. Saliency显著性

显著性方法识别输入数据的哪些属性与模型的预测或潜在表示最相关。在这一类中，涉及到人工检查，以决定一个显著性地图是否合理。显著性地图是有用的，也就是说，如果北极熊总是出现在带有雪或冰的图片中，该模型可能会滥用冰雪的信息来探测北极熊，而不是北极熊的真实特征。有了一个显著性映射，就可以找到这个问题，从而避免了这个问题。

显著性方法在可解释性研究中很流行，然而，广泛的随机检验报告说，一些显著性方法可能是模型独立和数据独立的[3]；也就是说，一些方法提供的显著性映射与边缘检测器产生的结果高度相似。这是有问题的，因为这意味着这些显著性方法无法找到能够解释模型预测的输入的真实属性。因此，在这些情况下，应开发一种与模型相关和与数据相关的显著性方法。



#### 4. Proxy代理

代理方法构造了一个更简单、更可解释的代理，它非常类似于一个经过训练的、大型的、复杂的和黑箱模型。代理方法可以在局部空间中是局部的，也可以在整个解空间中是全局的。示例性的代理模型包括决策树、规则系统等。代理方法的缺点是构建代理模型所需的额外成本。

大约有三种方法可以作为原型的代理：

##### Direct extraction直接提取

直接提取的要点是直接从训练模型提取出来的模型中构建一个新的可解释模型，如决策树[92]、[191]或基于规则的系统。在规则提取方面，同时可以使用分解[151]和教学方法[146]、[172]。教学方法提取的规则具有与神经网络相似的输入-输出关系。这些规则并不对应于网络的权重和结构。例如，有效性区间分析（VIA）[118]提取了以下形式的规则：

如果（输入∈是一个超立方体），那么输入就属于某个类。

Setinono和Liu [151]根据激活值的接近程度聚类隐藏单位激活值。然后，将每个聚类的激活值用它们的平均激活值来表示，同时保持神经网络的准确性尽可能的完整。接下来，输入数据与相同的平均隐藏单元激活值聚类在一起，得到一套完整的规则。在图7中，我们使用Setinono和Liu的方法在Iris数据集上说明了从单隐层网络中得到的规则。在二元分类问题的神经网络中，决策边界将输入空间划分为两部分，分别对应两类。在[146]中开发的解释系统HYPINV为每个决策边界超平面计算了一个切向量。输入实例和切向量之间的内积的符号将表示输入实例相对于决策边界的位置。基于这一事实，就可以建立起一个规则系统。

最后，一些专门的网络，如ANFIS [80]和RBF网络[126]，直接对应于模糊逻辑系统。例如，一个RBF网络等价于高木-Sugeno规则系统[171]，该系统包含规则，例如“如果x∈设置A和y∈集B，那么z=f（x，y）”[136]。[48]中的模糊逻辑解释将网络中的每个神经元/滤波器视为一个广义的模糊逻辑门。从这个观点来看，一个神经网络只不过是一个深度模糊的逻辑系统。具体来说，他们分析了一种新的神经网络，称为二次网络，其中所有的神经元都是二次神经元，用二次运算[47]代替内积。他们的解释广义为由二次神经元实现的模糊逻辑门，然后基于网络中模糊操作的频谱信息计算熵。结果表明，这种熵与最小值特性和神经网络的复杂性有很深的联系。



###### [151] Understanding neural networks via rule extraction

根据网络的权值使用剪枝算法去除网络的连接，最终目标是获得一组描述分类过程的简单规则。

当网络修剪完成后，网络只包含那些显著的连接。然而，规则并不容易提取，因为隐藏的单元激活值是连续的。这些值的离散化为规则提取铺平了道路。通过算法离散激活值（许多聚类算法可以用于此目的）。

经过网络剪枝和激活值离散化后，可以通过检查网络输出中可能的组合来提取规则。实际的规则提取是通过一种生成100%准确的规则的算法来完成的[Liu，1995]。然而，当一个隐藏单元和输入单元之间仍然有太多的连接（例如，超过7个）时，所提取的规则可能不容易理解。可以采用另外三层前馈子网络来简化隐藏单元的规则提取。该子网的训练方式与原始网络相同，但规模减少：输出单位的数量是隐藏单元的离散值的数量，而输入单位是原始网络中隐藏单元的连接数量。

理解神经网络是通过能够根据规则解释每个预测是如何进行的，以及通过从中生成规则来理解决策树来实现的。

###### [146] Neural network explanation using inversion

HYPINV是一种新的依赖于网络反演的解释算法，即计算产生所需输出的神经网络输入。

HYPINV以超平面的连接和分离的形式找到网络决策边界，即它以分段形式逼近神经网络决策边界。分段形式不是直接从数据中产生的，因为我们的目的是解释人工神经网络的行为。直接从数据中生成的规则可能是正确的，但不会反映问题的神经网络映射。对于连续或二进制属性神经网络，它可以用超平面来近似网络决策面。大多数从具有连续属性的神经网络中提取规则的算法都将它们转换为二进制形式，这对应于使用超立方体来近似决策区域。



说白了这几个玩意就是决策树。。。





##### Knowledge distillation知识蒸馏

第二种方法称为知识蒸馏[23]，如图8所示。虽然知识蒸馏技术主要用于模型压缩，但其原理也可用于可解释性。知识蒸馏的主题是，繁琐的模型可以生成相对准确的预测，将概率分配给所有可能的类，称为软标签，比一个热标签信息更丰富。例如，一匹马更有可能被归类为一只狗，而不是一个山。但是，使用一个热标签，狗类和山地类的概率都为零。[23]的结果表明，通过对原始模型的对数进行匹配，可以将原始繁琐模型的泛化能力转移到更简单的模型中。沿着这个方向，我们开发了一个可解释的代理模型，如一个决策树[38]、[185]、一个决策集[98]、一个全局加性模型[170]和一个更简单的网络[75]。例如，谭等[170]使用软标签训练全球加法模型的形式h0+我嗨（xi）+我=j嗨（xi，xj）+我=j j =k劫持（xi，xj，xk）+···，{嗨}我≥1也可以直接作为一个特性显著性地图。

知识蒸馏是通过原始复杂模型的软标签构建一个可解释的代理。





##### Provide a local explainer as a proxy提供一个局部解释器作为代理

局部解释器方法局部模拟了神经网络的预测行为。其基本原理是，当一个神经网络被全局检查时，它看起来很复杂。然而，如果我们在本地处理它，情况就会变得更加清晰。一个典型的局部解释器是局部可解释建模无关解释（LIME）[141]，它通过将样本的元素随机设置为0并计算相应的结果来合成一些邻居实例。然后，使用一个线性回归因子来拟合综合实例，其中线性模型的系数表示特征的贡献。如图9所示，将LIME方法应用于乳腺癌分类模型，以确定哪些属性是对该模型的良恶性预测的贡献力量。

Zhang等人[206]指出，LIME解释缺乏鲁棒性，这源于抽样方差、对参数选择的敏感性以及不同数据点之间的差异。锚[142]是LIME的改进扩展，它是寻找最重要的输入的片段，使其余段的可变性并不重要。数学上，Anchor搜索一个集合：一个={z|f(z)=f(x)，z∈x}，其中f（·）是一个黑盒模型，x是输入，z是x的一部分。另一个基于本地规则的解释（LORE）的建议来自[64]。LORE利用遗传算法来生成平衡的邻居，而不是随机的邻居，从而产生高质量的训练数据，从而减轻了LIME的采样方差。



###### LIME算法

https://github.com/marcotcr/lime

[本地可解释模型不可知解释（LIME）：简介 - O'Reilly (oreilly.com)](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)





#### 5. Advanced mathematical/physical analysis高级数学/物理分析

高级数学/物理分析方法将神经网络置于理论数学/物理框架中，在该框架中，可以使用高级数学/物理学工具了解神经网络的机制。这类研究涵盖了深度学习的理论进展，包括非凸优化、表征能力和泛化能力。

这类研究中所关注的一个问题是，为了建立一个合理的解释，有时会做出不切实际的假设，以促进理论分析，这可能会损害解释的实际有效性。



高级数学/物理分析： Lu等[114]表明，许多残差网络可以解释为常微分方程的离散数值解；即，ResNet [69]中残差块的内部工作可以建模为un+1 = un + f（un），其中un是第n个块的输出，f（un）是块运算。我们注意到，un+1 = un + f（un）是一个常微分方程du/dt = f (u)的一步有限差分近似。这个想法激发了ODE-Net [32]的发明。如图10所示，起点和动力学通过一个ODE-Net进行调整，以拟合一个螺旋。

###### ONE-net

- 博客 [神经常微分方程--ODEnet](https://www.cnblogs.com/lulixin/p/10165565.html)
- ODE-net 的 PyTorch 实现地址 https://github.com/rtqichen/torchdiffeq
- ODE-net 论文地址 https://arxiv.org/abs/1806.07366



Lei等人[101]在瓦瑟斯坦生成对抗网络（WGAN [8]）和最优运输理论之间建立了一个优雅的联系。他们得出的结论是，对于低维假设和有意设计的距离函数，生成器和鉴别器可以以一种封闭的形式精确地相互表示彼此。因此，在WGAN的训练中，鉴别器和发生器之间的竞争是不必要的。

在[153]中，提出了神经网络的学习是提取与输出随机变量Y相关的输入随机变量X中最相关的信息。对于前馈神经网络，互信息的不等式如下：（自己看论文）

其中I（·；·）表示互信息，hi和hj是隐藏层的输出（i > j表示第i个层更深），ˆY是最终的预测。此外，Yu和普林西比·[197]采用信息瓶颈理论来测量堆叠自动编码器中对称层的互信息状态，如图11所示。然而，由于数据的概率分布通常是先验未知的，因此估计互信息是很棘手的。

Kolouri等人[91]用广义Radon变换建立了神经网络的积分几何解释。让X是一个随机变量的输入，符合分布pX，然后我们可以得出一个概率分布函数的输出神经网络fθ (X)参数化θ： pfθ (z) =X pX (x)δ（z−fθ (x)）dx，广义氡变换，超曲面是H（t，θ）= {x∈X|fθ (x) = t}。在这方面，神经网络的变换以扭曲的超曲面为特征。Huang [77]利用平均场理论来表征一个深度网络的降维机制，该网络在每一层都假设权值，输入数据遵循高斯分布。在他的研究中，将第1层输出的自协方差矩阵计算为Cl，然后将内在维数定义为D = [（（Ni=1 λi）2）/（Ni=1 λ2i）]，其中λi为Cl的特征值，N为特征值的个数。研究了跨层的数量D/N，以分析如何跨层学习紧凑表示。Ye等人[192]利用框架理论和低秩汉克尔矩阵，用局部和非局部基来表示信号，对应于卷积和广义池化运算。然而，在他们的研究中，网络结构被简化为将两个ReLU单元连接成一个线性单元，从而可以绕过来自ReLU单元的非线性。就先进的物理模型而言，Mehta和Schwab [121]建立了从卡达诺夫变分重整化群[82]到限制玻尔兹曼机（RBM）[147]的精确映射。这种映射独立于能量函数的形式，可以缩放到任何RBM。

###### Radon变换

- [Radon 变换原理和应用](https://blog.csdn.net/deepsprings/article/details/119321544?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-119321544-blog-79504530.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-3-119321544-blog-79504530.pc_relevant_aa&utm_relevant_index=5)



理论神经网络研究对可解释性也至关重要。目前，深度学习的理论基础主要从三个方面出发：1）表示；2）优化；以及3）泛化。

表示：让我们在这里包括两个例子。第一个例子是要解释为什么深层网络优于浅层网络。认识到深度网络的成功，科恩等人[37]、埃尔丹和沙米尔[44]、梁和斯里特[109]、马斯卡和Poggio [124]，以及斯曼斯基和麦肯[169]证明，深度网络比浅网络更有表现力。最基本的思想是构造一类特殊的函数，它可以有效地用深网络表示，但很难用浅网络逼近。第二个例子是了解深度网络的快捷连接的效用。Veit等人[177]表明，残余连接可以使神经网络表现出类似集合的行为。沿着这个方向，[110]报道，有了捷径，网络可以超薄，以允许通用近似。

优化：优化一个深度网络通常是一个np困难的非凸问题。鞍点[56]的普遍存在导致了即使找到一个局部最小值也是np-硬[5]。我们特别感兴趣的是，为什么一个过参数化的网络仍然可以很好地优化，因为一个深度网络是一种过参数化的网络。过度参数化网络的特点是，网络中的参数的数量远远超过了数据实例的数量。[162]等人表明，当数据是高斯分布的，神经元的激活函数是二次的时，一个过参数化的单隐层网络的景观可以有效地搜索全局最优。Nguyen和Hein [130]证明，对于线性可分数据，在前馈神经网络的权值矩阵的秩假设下，损失函数的每个临界点都是一个全局最小值。此外，Jacot等人[78]指出，当神经网络中每一层的神经元数量无限大时，训练只对网络函数产生很小的变化。因此，对网络的训练变成了核岭回归。

泛化：传统的泛化理论不能解释为什么深度网络可以很好地泛化，尽管深度网络的参数数量远远超过样本的数量。最近提出的依赖于权值矩阵范数的泛化界[127]部分解决了这个问题。然而，这些边界对数据有异常的依赖，而更多的数据会导致更大的泛化边界，这显然与常识相矛盾。我们认为，还需要更多的努力来令人满意地解决泛化难题，[18]，[122]。





#### 6. Explaining-by-Case案例解释

案例解释方法是基于案例推理[90]的。人们喜欢的例子。一个人可能不会被一个产品的无聊的统计数字所吸引，但在听到其他用户使用这种产品的体验时，他可能会感到惊讶。这一哲学赢得了许多从业者的心，并吸引了深度学习的基于案例的解释。按案例解释的方法提供了捕获模型本质的代表性示例。

这类方法既有趣又鼓舞人心。然而，这种做法更像是一种完整性检查，而不是一种一般的解释，因为从选定的查询用例中并没有理解到很多关于神经网络的内部工作的信息。



按案例解释：基本上，基于案例的解释呈现了一个被神经网络认为与需要解释的查询案例最相似的案例。找到一个类似的案例来解释，并从数据中选择一个有代表性的案例作为原型[19]基本上是相同的，只是使用不同的度量来衡量相似性。原型选择是找到可以代表整个数据集的实例的最小子集，而基于案例的解释使用基于神经网络表示的接近度的相似性度量，从而暴露隐藏的表示信息。因此，基于案例的解释也与深度度量学习[149]有关。

如图12所示，华莱士et al. [180]采用k最近邻算法获得最相似的情况下查询情况的特征空间，然后计算最近的邻居属于预期类的可解释性，表明多少预测是支持的数据。Chen等人[31]构建了一个模型，可以通过寻找原型部分来解剖图像。具体来说，模型的管道在卷积层之后分裂成多个通道，其中每个通道的函数被期望学习输入的一个原型部分，如鸟的头部或身体。输入图像的决策是基于信道特征的相似性。

Wachter等人[179]提供了一种新的基于案例的解释方法，提供了一个反事实案例，这是一个虚构的案例，接近查询，但与查询的输出不同。反事实的解释提供了所谓的“最接近的可能的情况”或最小的变化，以产生不同的结果。例如，反事实的解释可能会产生以下的说法：“如果你有一个好的前锋，你的球队就会赢得这场足球比赛。”巧合的是，为了实现“对抗性扰动”，已经发展出了产生反事实解释的技术；即结构攻击[190]。本质上，找到最接近输入x的可能情况x，相当于找到对x的最小扰动，从而使分类结果发生变化。例如，可以构建以下优化过程：



其中λ是一个常数，y是另一个标签，并选择d（·，·）为曼哈顿距离，希望输入受到最小的扰动。Goyal等人[62]探索了另一种得出反事实视觉解释的方法。给定一个图像我标签c，因为反事实视觉解释代表了输入的变化可以迫使模型产生不同的预测类c，他们选择了一个图像我标签c和设法识别空间区域我和我识别区域的替换将改变模型预测从c到c。



###### 反事实解释

反事实counterfactual是对已有结果进行假设，再推理，估计其中一项影响因素的发生概率。近年来在机器学习广泛应用，指代与因果分析相关的广泛性技术。

**典型实例：**

1. **已知 Alice 在工作中没有得到晋升，已知她是一名女性，已知我们可以观察到的关于她的情况和表现的一切事情，那么，如果 Alice 是男性，她的晋升概率又会如何？**

   我们问这个问题，目的是在检测性别对结果有多大的影响。但是我们在这里，需要明确的一点就是，这是一个单独的公平性概念，不是整个模型的公平性。也就是说，我们不知道模型是否公平或者我们假设模型是相对公平的。我们只是评估这个特例里面，Alice是男性，会对结果产生的影响。

2. **假设我获得硕士学位，并且在此期间我留了很长的胡子，假设我没有留胡子，我还能获得硕士学位吗？**

   这里我已经获得硕士学位，并且我留有胡子。我想知道胡子对于我获得学位有多大影响，但事实是，有没有胡子，我都会获得学位。这里评估的是胡子对于获得学位的影响概率。

**数学表示**

回到Alice的例子，将其假设为两个例子：

1. Alice是女性，在工作中兢兢业业；
2. Alice是男性，其它信息和上一个例子完全一致。

则有
	P1(晋升|Alice=女)和 P2(晋升|Alice=男)

这两个概率之间，如果结果一致，我们就可以认为晋升和性别无关，如果结果不一致，我们就将P1和P2的差值，当作性别在模型中的影响概率。这是非常好理解的，但是这只是一个结果概率。

**结论**

反事实counterfactual通常被认为是不科学的，因为这不是在经验上可测得的。在机器学习中，我们可以在测试数据集上对模型进行测试，但那是在因果机器学习中，不是所有东西都可以直接测试或者根据已有经验进行测试的，毕竟过去的事情不可以重来。简而言之，就是因果机器学习只是假设既定事实中影响结果的某一因素，但是我们没办法验证这个因素在现实中会对最终结果产生的影响。
但是反事实技术可以给了解或者使用模型相关人员外的决策者一个参考，比如政客的军师通过研究往届选举人的特点，分析某些因素对选举结果的影响。比如间谍通过收集敌方情报，研究某一因素是否对自己国家有强威胁。



#### 7. Explaining-by-Text文本解释

文本解释方法在有利于理解模型行为的图像语言联合任务中生成的文本描述。这个类还可以包括生成用于解释的符号的方法。

这类方法在图像语言联合任务中特别有用，例如从x射线x线片中生成诊断报告。然而，用文本来解释并不是任何深度学习模型的通用技术，因为它只能在模型中存在语言模块时工作。





### Ad-Hoc Interpretable Modeling

临时可解释性建模消除了在事后可解释性分析中或多或少存在的偏差。虽然一般认为在可解释性和模型可表达性[123]之间存在权衡，但仍然有可能找到一个既强大又可解释的模型。一个显著的例子是在[30]中报告的工作，其中一个可解释的双层加性风险模型在FICO识别竞赛中获得了第一名。



#### 8. Interpretable representation可解释的表示

可解释表示方法采用正则化技术来引导神经网络的优化走向更可解释的表示。诸如可分解性、稀疏性和单调性等性质可以增强可解释性。因此，正则化特性成为了允许更多可解释的模型的一种方法。相应地，为了实现可解释性，损失函数必须包含一个正则化术语，这限制了原始模型执行其完整的学习任务。



#### 9. Model renovation模型改造

模型改造方法通过设计和部署更多可解释的机制来寻求可解释性。这些机制包括具有故意设计的激活功能的神经元、具有特殊功能的插入层、模块化架构等等。未来的发展方向是使用越来越多的可解释的组件，同时可以为不同的任务实现类似的最先进的性能。











